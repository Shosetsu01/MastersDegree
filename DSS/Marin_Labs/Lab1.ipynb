{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Одномерная оптимизация - реализовать метод и добавить к нему метод свена, чтобы не вручную задавать диапазон поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция sven_method(x0, h, f) реализует метод Свена для нахождения интервала, на котором функция возрастает или убывает.\n",
    "\n",
    "Метод Свена (Sven's method) - это итерационный численный метод, который используется для нахождения интервала, на котором функция возрастает или убывает. Этот метод полезен для последующего применения других методов оптимизации, таких как метод дихотомии или метод золотого сечения.\n",
    "\n",
    "Принцип работы метода Свена следующий:\n",
    "\n",
    "Начинаем с заданной точки x0 и шага h.\n",
    "Вычисляем значения функции в трех точках: слева от x0 на расстоянии h, в самой точке x0 и справа от x0 на расстоянии h. Обозначим эти значения как left_f_x0, f_x0 и right_f_x0 соответственно.\n",
    "\n",
    "Проверяем условия относительно значений функции:\n",
    "Если выполняется условие left_f_x0 >= f_x0 <= right_f_x0, то считаем, что функция возрастает на интервале [x0 - h, x0 + h], и возвращаем этот интервал.\n",
    "Если выполняется условие left_f_x0 <= f_x0 >= right_f_x0, то считаем, что функция убывает на интервале [x0 - h, x0 + h], и возвращаем None.\n",
    "Если выполняется условие left_f_x0 >= f_x0 >= right_f_x0, то функция может иметь локальный минимум внутри интервала. В этом случае выбираем новую точку x1 = x0 + h.\n",
    "Если выполняется условие left_f_x0 <= f_x0 <= right_f_x0, то функция может иметь локальный максимум внутри интервала. В этом случае выбираем новую точку x1 = x0 - h, а шаг h меняем на противоположный, т.е. h = -h.\n",
    "Удваиваем шаг h, чтобы увеличить длину интервала.\n",
    "Запускаем цикл, в котором сравниваем значение функции в точке x1 с значением в точке x0.\n",
    "Если функция в точке x1 меньше функции в точке x0, обновляем x0 = x1, а затем выбираем новую точку x1 путем прибавления шага h к x0.\n",
    "Удваиваем шаг h, чтобы увеличить длину интервала.\n",
    "Продолжаем этот процесс до тех пор, пока функция в точке x1 остается меньше функции в точке x0.\n",
    "После выхода из цикла проверяем знак шага h. Если `h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sven_method(x0, h, f):\n",
    "    left_f_x0, f_x0, rigth_f_x0 = f(x0 - h), f(x0), f(x0 + h)\n",
    "    if left_f_x0 >= f_x0 <= rigth_f_x0:\n",
    "        return (x0 - h, x0 + h)\n",
    "    if left_f_x0 <= f_x0 >= rigth_f_x0:\n",
    "        return None \n",
    "    if left_f_x0 >= f_x0 >= rigth_f_x0:\n",
    "        x1 = x0 + h \n",
    "    if left_f_x0 <= f_x0 <= rigth_f_x0:\n",
    "        x1 = x0 - h\n",
    "        h = -h\n",
    "    h *= 2\n",
    "    while f(x1) < f(x0):\n",
    "        tmp = x0 \n",
    "        x0 = x1\n",
    "        x1 = x0 + h\n",
    "        h *= 2\n",
    "\n",
    "    if h > 0:\n",
    "        return(tmp, x1)\n",
    "    return (x1, tmp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция dichotomy_method(E, l, f, a, b) реализует метод дихотомии для нахождения минимума функции на заданном интервале [a, b].\n",
    "\n",
    "Метод дихотомии (или метод деления отрезка пополам) - это итерационный численный метод, который используется для нахождения минимума (или максимума) функции на заданном интервале. Этот метод основан на принципе деления интервала пополам и последующем сужении этого интервала до требуемой точности.\n",
    "\n",
    "Принцип работы метода дихотомии следующий:\n",
    "\n",
    "Начинаем с заданного интервала [a, b], на котором предполагается нахождение минимума функции.\n",
    "Задаем требуемую точность E - разницу между границами интервала, при которой алгоритм останавливается.\n",
    "Задаем начальную длину шага l, которая определяет точность поиска минимума.\n",
    "Запускаем цикл, который будет выполняться до тех пор, пока разница между границами интервала [a, b] больше заданной точности E.\n",
    "На каждой итерации цикла вычисляем две промежуточные точки:\n",
    "y = (a + b - l) / 2 - левая промежуточная точка\n",
    "z = (a + b + l) / 2 - правая промежуточная точка\n",
    "Сравниваем значения функции в точках y и z. Если функция в точке y больше значения в точке z, то минимум функции находится справа от точки y, и мы обновляем левую границу интервала: a = y. В противном случае, минимум находится слева от точки z, и мы обновляем правую границу интервала: b = z.\n",
    "Повторяем шаги 5-6 до тех пор, пока разница между границами интервала [a, b] не станет меньше заданной точности E.\n",
    "По окончании цикла получаем интервал [a, b], в котором находится минимум функции с требуемой точностью.\n",
    "Возвращаем среднее значение границ интервала (a + b) / 2 в качестве приближенного значения минимума функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dichotomy_method(E, l, f, a, b):\n",
    "\n",
    "    while abs(a - b) > l:\n",
    "        y = (a + b - E) / 2\n",
    "        z = (a + b + E) / 2\n",
    "        if f(y) > f(z):\n",
    "            a = y\n",
    "        else:\n",
    "            b = z\n",
    "\n",
    "    return (a + b) / 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция steepest_gradient_descent_method(x, e1, e2, M, f, plot=False) реализует метод наискорейшего спуска для оптимизации функции.\n",
    "\n",
    "Метод наискорейшего спуска (или градиентного спуска) является одним из наиболее популярных методов оптимизации функций. Он используется для нахождения минимума функции путем последовательного движения в направлении, обратном градиенту функции.\n",
    "\n",
    "Принцип работы метода наискорейшего спуска следующий:\n",
    "\n",
    "1. Задаем начальную точку `x`, с которой начинается поиск минимума функции.\n",
    "2. Задаем требуемую точность `e1` для сходимости алгоритма (норма градиента функции).\n",
    "3. Задаем требуемую точность `e2` для остановки алгоритма (разница между значениями функции на текущей и предыдущей итерации).\n",
    "4. Задаем максимальное количество итераций `M`, чтобы избежать бесконечного цикла.\n",
    "5. Вычисляем градиент функции `g_f` в текущей точке `x`.\n",
    "6. Запускаем цикл, который будет выполняться до выполнения одного из следующих условий:\n",
    "   - Норма градиента `g_f` становится меньше требуемой точности `e1`.\n",
    "   - Происходит несущественное изменение значений функции, то есть разница между значениями функции на текущей и предыдущей итерации становится меньше требуемой точности `e2`.\n",
    "   - Превышено максимальное количество итераций `M`.\n",
    "7. На каждой итерации цикла вычисляем шаг `t` в направлении антиградиента функции `g_f`.\n",
    "   - Часто используется метод одномерной оптимизации (например, метод дихотомии или метод золотого сечения), чтобы выбрать оптимальное значение шага `t`, которое минимизирует функцию в направлении спуска.\n",
    "8. Обновляем текущую точку `x`:\n",
    "   - `x = x - t * g_f` - двигаемся в направлении антиградиента с оптимальным шагом.\n",
    "9. Повторяем шаги 5-8 до выполнения условия остановки.\n",
    "10. Возвращаем найденную точку `x` в качестве приближенного значения минимума функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_gradient_descent_method(x, e1, e2, M, f, plot=False,):\n",
    "    dx = 0.0000001\n",
    "    df = lambda x : grad_f(dx, x, f)\n",
    "    array_dots = []\n",
    "    k = 0\n",
    "    g_f = df(x)\n",
    "    while np.linalg.norm(g_f, ord=2)  >= e1 and k < M:\n",
    "        t = f_t(x, g_f, f)\n",
    "        tmp = x\n",
    "        x = x - t * g_f\n",
    "        if plot:\n",
    "            array_dots.append(x.copy())\n",
    "        if np.linalg.norm(x - tmp, ord=2) < e2  and abs( f(x) - f(tmp)) < e2:\n",
    "            break\n",
    "        k +=1\n",
    "        g_f = df(x)\n",
    "    return x\n",
    "    \n",
    "\n",
    "\n",
    "def grad_f(dx, x, f):\n",
    "    array_x = []\n",
    "    for i in range(len(x)):\n",
    "        tmp  = x.copy()\n",
    "        tmp[i] = tmp[i] - dx\n",
    "        tmp2 = x.copy()\n",
    "        tmp2[i] = tmp2[i] + dx\n",
    "        array_x.append([tmp2, tmp])\n",
    "    array_x = np.array(array_x)\n",
    "    tmp = [(f(cloum[0]) - f(cloum[1])) / (2 * dx) for cloum in array_x]\n",
    "    return np.array(tmp)\n",
    "\n",
    "def f_t(x, g_f, f,):\n",
    "    t = lambda t: f(x - g_f * t)\n",
    "    E = 0.000000001\n",
    "    l = 0.00001\n",
    "    random.seed( version=2)\n",
    "    h = random.uniform(0.1, 1000)\n",
    "    x0 = random.uniform(-1000, 1000)\n",
    "    segment = sven_method(x0, h, t)\n",
    "    if segment:\n",
    "        a, b = segment \n",
    "    ans = dichotomy_method(E, l, t, a, b)\n",
    "    return ans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код запуска программы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.22046967  0.39960861]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    n = 2\n",
    "    dx = 0.005\n",
    "    f = lambda x : 2 * x[0] ** 2 + x[0] * x[1] + x[1] ** 2  \n",
    "    df = lambda x : grad_f(dx, x, f)\n",
    "    x0 = [0.5, 1.0]\n",
    "    x0 = np.array(x0)\n",
    "    e1 = 0.1\n",
    "    e2 = 0.15\n",
    "    M = 1\n",
    "    ans = steepest_gradient_descent_method(x0, e1, e2, M, f)\n",
    "    print(ans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
